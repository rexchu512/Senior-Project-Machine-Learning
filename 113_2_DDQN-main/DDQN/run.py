import pickle
import time
import os
import numpy as np
import pandas as pd
import argparse    #è§£æå‘½ä»¤è¡Œåƒæ•¸
import re          #æ­£å‰‡è¡¨é”å¼ï¼Œé€™è£¡ç”¨æ–¼å¾æ¨¡å‹æª”æ¡ˆåæå–æ™‚é–“æˆ³
import itertools

from envs import TradingEnv
from agent import DDQNAgent
from utils import get_data, get_scaler, maybe_make_dir, plot_all

#è®€å–ç‰¹å®šè³‡æ–™é›†
stock_name = "PFdata"
stock_table = "PFtable"
CLI = pd.read_csv('data/CLI.csv'.format(stock_name)).drop(columns="DateTime")
CPI = pd.read_csv('data/CPI.csv'.format(stock_name)).drop(columns="DateTime")
Initial = pd.read_csv('data/Initial.csv'.format(stock_name)).drop(columns="DateTime")
IPI = pd.read_csv('data/IPI.csv'.format(stock_name)).drop(columns="DateTime")
Manufacturing = pd.read_csv('data/Manufacturing.csv'.format(stock_name)).drop(columns="DateTime")
Unemployment = pd.read_csv('data/Unemployment.csv'.format(stock_name)).drop(columns="DateTime")


#å®šç¾©éœ€è¦çš„å‘½ä»¤åˆ—åƒæ•¸
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-e', '--episode', type=int, default=2,
                        help='number of episode to run')
    parser.add_argument('-b', '--batch_size', type=int, default=32,
                        help='batch size for experience replay')
    parser.add_argument('-i', '--initial_invest', type=int, default=20000,
                        help='initial investment amount')
    parser.add_argument('-m', '--mode', type=str, required=True,
                        help='either "train" or "test"')
    #å·²è¨“ç·´å¥½çš„æ¨¡å‹æ¬Šé‡ï¼ˆç”¨æ–¼æ¸¬è©¦æ¨¡å¼ï¼‰
    parser.add_argument('-w', '--weights', type=str, help='a trained model weights')
    args = parser.parse_args()

    print(args)

    maybe_make_dir('weights')
    maybe_make_dir('portfolio_val')

    """è®€å–è‚¡ç¥¨è³‡æ–™"""
    timestamp = time.strftime('%Y%m%d%H%M')

    # æŠŠ data è½‰æˆ np æ‰€ä»¥æ™¯æ°£ä¹Ÿè¦è·Ÿè‘—è½‰
    data = get_data(stock_name, stock_table)
    CLI = np.array(CLI.T)
    CPI = np.array(CPI.T)
    Initial = np.array(Initial.T)
    IPI = np.array(IPI.T)
    Manufacturing = np.array(Manufacturing.T)
    Unemployment = np.array(Unemployment.T)
    train = round(data.shape[1]*0.75)
    # test = round(data.shape[1]*0.99)
    # train = 979
    test = 34
    # print("train:{}, test:{}".format(data[:, train-1], data[:, test]))
    train_data = data[:, :-test]
    test_data = data[:, -test:]
    CLI_train = CLI[:, :-test]
    CLI_test = CLI[:, -test:]
    CPI_train = CPI[:, :-test]
    CPI_test = CPI[:, -test:]
    Initial_train = Initial[:, :-test]
    Initial_test = Initial[:, -test:]
    IPI_train = IPI[:, :-test]
    IPI_test = IPI[:, -test:]
    Manufacturing_train = Manufacturing[:, :-test]
    Manufacturing_test = Manufacturing[:, -test:]
    Unemployment_train = Unemployment[:, :-test]
    Unemployment_test = Unemployment[:, -test:]

    # âœ… å¦‚æœæ˜¯ Grid Search æ¨¡å¼
    if args.mode == "grid-search":
        param_grid = {
            "gamma": [0.99, 0.95, 0.90],  # æŠ˜æ‰£å› å­
            "learning_rate": [0.001, 0.01, 0.1],  # å­¸ç¿’ç‡
            "replay_memory_size": [1000, 2000, 5000],  # å›æ”¾è¨˜æ†¶é«”å¤§å°
            "epsilon_decay": [0.995, 0.99, 0.98],  # æ¢ç´¢ç‡è¡°æ¸›
            "batch_size": [32, 64, 128]  # å¢åŠ  batch_size
        }

        param_combinations = list(itertools.product(*param_grid.values()))
        total_tests = len(param_combinations)  # âœ… è¨ˆç®—ç¸½æ¸¬è©¦æ•¸é‡
        best_reward = -np.inf
        best_params = None
        results = []

        for idx, params in enumerate(param_combinations, start=1):
            gamma,learning_rate, replay_memory_size, epsilon_decay, batch_size = params
            print(f"\nGrid Search {idx}/{total_tests} æ¸¬è©¦ä¸­...")
            print(f"gamma={gamma}, learning_rate={learning_rate}, replay_memory_size={replay_memory_size}, epsilon_decay={epsilon_decay}, batch_size={batch_size}")

            env = TradingEnv(train_data, CLI_train, CPI_train, Initial_train, IPI_train, Manufacturing_train, Unemployment_train, args.initial_invest)
            state_size = env.observation_space.shape
            action_size = env.action_space.n

            agent = DDQNAgent(
                state_size, action_size,
                gamma=gamma,
                replay_memory_size = replay_memory_size,
                epsilon_decay=epsilon_decay)

            scaler = get_scaler(env)
            total_rewards = []

            for e in range(args.episode):  # æœªæ¸›å°‘å›åˆæ•¸ï¼ŒåŠ å¿« Grid Search
                state = env.reset()
                state = scaler.transform([state])
                episode_reward = 0

                for _ in range(env.n_step):
                    action = agent.act(state)
                    next_state, reward, done, info = env.step(action)
                    next_state = scaler.transform([next_state])
                    agent.remember(state, action, reward, next_state, done)
                    state = next_state
                    # æª¢æŸ¥ reward æ˜¯å¦ç‚º NaN
                    if np.isnan(reward):
                        reward = 0  
                    episode_reward += reward

                    if done:
                        break

                agent.replay()  # âœ… batch_size å…§éƒ¨è™•ç†
                total_rewards.append(episode_reward)

            avg_reward = np.mean(total_rewards)
            results.append((gamma, learning_rate, replay_memory_size, epsilon_decay, batch_size, avg_reward))  # âœ… ä¿®æ­£è®Šæ•¸æ•¸é‡

            print(f"   âœ… æ¸¬è©¦å®Œæˆï¼å¹³å‡çå‹µ: {avg_reward}")

            if avg_reward > best_reward:
                best_reward = avg_reward
                best_params = params

        df_results = pd.DataFrame(results, columns=["gamma", "learning_rate", "epsilon_min", "epsilon_decay", "batch_size", "avg_reward"])
        

        # å»ºç«‹ 'results' è³‡æ–™å¤¾
        save_dir = os.path.join(os.getcwd(), "results")
        os.makedirs(save_dir, exist_ok=True)
        # å„²å­˜ CSV æ–‡ä»¶æ–¼æ–°è³‡æ–™å¤¾
        file_path = os.path.join(save_dir, "grid_search_results.csv")
        df_results.to_csv(file_path, index=False)
        
        print("\nğŸ¯ Grid Search å®Œæˆï¼")
        print(f"ğŸ¯ æœ€ä½³è¶…åƒæ•¸çµ„åˆ: {best_params}")
        print(f"ğŸ¯ æœ€é«˜å¹³å‡çå‹µ: {best_reward}")
        exit()

    # é€² env å®šç¾© business_cycle
    env = TradingEnv(train_data, CLI_train, CPI_train, Initial_train, IPI_train, Manufacturing_train, Unemployment_train, args.initial_invest)
    #åˆå§‹åŒ–ç’°å¢ƒèˆ‡ä»£ç†
    state_size = env.observation_space.shape
    action_size = env.action_space.n
    agent = DDQNAgent(state_size, action_size)
    scaler = get_scaler(env)

    #è¨“ç·´æˆ–æ¸¬è©¦æ¨¡å¼
    portfolio_value = []

    if args.mode == 'test':
        # ä½¿ç”¨æ¸¬è©¦æ•¸æ“šé‡æ–°è£½ä½œç’°å¢ƒ
        # åŠ å…¥æ™¯æ°£å‡½æ•¸?
        env = TradingEnv(test_data, CLI_test, CPI_test, Initial_test, IPI_test, Manufacturing_test, Unemployment_test, args.initial_invest)        # è¼‰å…¥è¨“ç·´å¥½çš„æ¬Šé‡
        agent.load(args.weights)
        # æ¸¬è©¦æ™‚ï¼Œæ™‚é–“æˆ³èˆ‡è¨“ç·´æ¬Šé‡çš„æ™‚é–“ç›¸åŒ
        timestamp = re.findall(r'\d{12}', args.weights)[0]
        # daily_portfolio_value = [env.init_invest]
        daily_portfolio_value = []

    #é–‹å§‹è¨“ç·´æˆ–æ¸¬è©¦
    """
æ¯å€‹æ­¥é©Ÿä¸­ï¼Œä»£ç†é¸æ“‡ä¸€å€‹å‹•ä½œï¼Œç’°å¢ƒè¿”å›ä¸‹ä¸€å€‹ç‹€æ…‹ã€å›å ±ã€å®Œæˆæ¨™èªŒå’Œé¡å¤–è³‡è¨Š
åœ¨è¨“ç·´æ¨¡å¼ä¸‹ï¼Œä»£ç†æœƒè¨˜ä½ç¶“é©—ä¸¦é€²è¡Œç¶“é©—å›æ”¾
åœ¨æ¸¬è©¦æ¨¡å¼ä¸‹ï¼Œè¨˜éŒ„æ¯æ—¥çš„æŠ•è³‡çµ„åˆåƒ¹å€¼
ç•¶å›åˆçµæŸæ™‚ï¼Œè‹¥æ˜¯æ¸¬è©¦æ¨¡å¼ä¸”æ¯100å›åˆåŸ·è¡Œä¸€æ¬¡åœ–è¡¨ç¹ªè£½ï¼Œä¸¦æ‰“å°æ¯å€‹å›åˆçš„æœ€çµ‚çµæœ
    """
    for e in range(args.episode):
        state = env.reset()
        state = scaler.transform([state])
        for time in range(env.n_step):
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            next_state = scaler.transform([next_state])
            if args.mode == 'train':
                agent.remember(state, action, reward, next_state, done)
            if args.mode == "test":
                daily_portfolio_value.append(info['cur_val'])
            state = next_state
            if done:

                if args.mode == "test" and e % 100 == 0:
                    plot_all(stock_name, daily_portfolio_value, env, test-1)#-1ï¼Œstep 1å¾Œæ‰é–‹å§‹è¨˜éŒ„cur_val

                daily_portfolio_value = []
                print("episode: {}/{}, episode end value: {}".format(
                    e + 1, args.episode, info['cur_val']))
                portfolio_value.append(info['cur_val']) # append episode end portfolio value

                break
            if args.mode == 'train' and len(agent.memory) > args.batch_size:
                agent.replay(args.batch_size)
        #if args.mode == 'train' and (e + 1) % 10 == 0:  # checkpoint weights
        if args.mode == 'train' and (e + 1) % 2 == 0:  # checkpoint weights
            #agent.save('weights/{}-dqn.h5'.format(timestamp))
            agent.save('weights/{}-dqn.weights.h5'.format(timestamp))

    #æœ€å¾Œè¼¸å‡ºçµæœèˆ‡ä¿å­˜æŠ•è³‡çµ„åˆåƒ¹å€¼æ­·å²è¨˜éŒ„
    print("mean portfolio_val:", np.mean(portfolio_value))
    print("median portfolio_val:", np.median(portfolio_value))

    # save portfolio value history to disk
    with open('portfolio_val/{}-{}.p'.format(timestamp, args.mode), 'wb') as fp:
        pickle.dump(portfolio_value, fp)
